# Things to check:
# Time of generation and time of ingetion (look at the difference)
# Can windows update happen? it can break fluent-bit/fluentd possibly
# Persistance (so that logs are continuous) and write position file to host file system
# when log entry is json, does it work?(proably needs a setting) -> #     Decode_Field_As   escaped    log -> find similar setting in fluentd?(parse json directive in input plugin)

<source>
  type heartbeat_request
  run_interval 30m
  @log_level info
</source>

<source>
  @type tail
  path /var/log/containers/*.log
  pos_file /var/opt/microsoft/fluentd/fluentd-containers.log.pos
  tag oms.container.log.la
  @log_level trace
  path_key tailed_path
  <parse>
	  @type  json
    time_format %Y-%m-%dT%H:%M:%S.%NZ
    keep_time_key true
  </parse>
</source>

<source>
  @type tail
  path  /var/log/containers/omsagent*.log
  pos_file /var/opt/microsoft/fluentd/omsagent-fluentd-containers.log.pos
  tag oms.container.log.flbplugin
  @log_level trace
  path_key tailed_path
  <parse>
	  @type  json
    time_format %Y-%m-%dT%H:%M:%S.%NZ
    keep_time_key true
  </parse>
</source>

<filter  oms.container.**>
  @type record_transformer
  # Is this safe to do(deleting and resuing in same plugin)? 
  # fluent-plugin-record-modifier more light-weight but needs to be installed (dependency worth it?)
  remove_keys tailed_path
  <record>
    filepath ${record["tailed_path"]}
  </record>
</filter>


<match oms.container.**>
  @type forward
  send_timeout 60s
  recover_wait 10s
  hard_timeout 60s
  heartbeat_type none
  ignore_network_errors_at_startup true
  # compress gzip # panic: reflect: call of reflect.Value.Index on int64 Value in fluent-bit
  <server>
    name logaggregationserver
    host 127.0.0.1
    port 25230
    weight 60
  </server>

  <buffer>
        overflow_action throw_exception
        chunk_limit_size 32k
        queued_chunks_limit_size 256
        flush_interval 1
        flush_thread_interval 0.5
        flush_thread_burst_interval 0.01
        flush_thread_count 4
        retry_forever true
    </buffer>
</match>